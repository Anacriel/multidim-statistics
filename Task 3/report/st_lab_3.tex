% XeLaTeX document
\documentclass[12pt, a4paper]{article}
\input{packages.tex}
\usepackage{fontspec}
\usepackage{diagbox}
\input{borders.tex}
\renewcommand{\baselinestretch}{1.4} 

\begin{document}

\input{title_page.tex}

\tableofcontents
\addtocontents{toc}{~\hfill\par}
\vfill ~
\setcounter{section}{0}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage 
\section*{Постановка задачи}
\addcontentsline{toc}{section}{Постановка задачи}

\indent{\indent В данной работе рассматривается задача классификации объектов на основе дискриминантного анализа. Пусть есть некоторые популяции $W_1, W_2$. Тогда задача состоит в отнесении объекта $w$ к одной из популяций на основе вектора признаков $X = {x_1, \dots, x_p}$. Необходимо построить и протестировать классификатор на разных данных:

\begin{enumerate}
	\item Модельные данные
	\item Данные из репозитория \\ \url{https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/}
\end{enumerate}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Ход работы}
\subsection{Построение дискриминантной функции}

\indent{\indent Сгенерируем обучающие выборки, представляющие популяции $W_1, W_2$, в предположении, что выборки имеют многомерное нормальное распределение:}

\[x \sim N\left(\mu_1, \sum\nolimits_1\right), \;\; x \sim N\left(\mu_2, \sum\nolimits_2\right)\],
где $\mu_1, \mu_2 \in R^p$ –– векторы средних, $\sum\nolimits_1 = \sum\nolimits_2 \in R^p \times R^p$ –– матрицы ковариаций.

\vspace{1cm}

\indent{Дискриминантная функция имеет вид}

\begin{equation}
	z(x) = \alpha_1 x_1 + \dots + \alpha_p x_p,
	\label{eq:discrim_func}
\end{equation}
где вектор коэффициентов $\alpha \in R^p$ находится по формуле:

\begin{equation}
	\alpha = \sum\nolimits^{-1} (\mu_1 - \mu_2)
	\label{eq:alpha}
\end{equation}

\indent{Наблюдение из $x_i$ из выборки будем относить к $W_1$, если $z \ge c$, и к $W_2$, если $z < c$, где $c$ –– постоянная.}

\indent{Константу $c$ будем вычислять по формулам:}

\begin{equation}
	c = \frac{(\xi_1 + \xi_2)}{2},
	\label{eq:c_formula}
\end{equation}
где $\xi_1 = \sum_{j=1}^p \alpha_j \mu_{1_j}, \;\; \xi_2 = \sum_{j=1}^p \alpha_j \mu_{2_j}$ –– средние $z$ для $x \in W_1, W_2$ соответственно, $\sigma_z^2 = \sum_{m = 1}^p \sum_{j = 1}^p \alpha_m \sigma_{mj} \alpha_j$ –– дисперсия

\vspace{1cm}

\indent{Для измерения "расстояния" между двумя популяциями воспользуемся расстоянием Махалонобиса:}

\begin{equation}
	\Delta^2 = \frac{(\xi_1 - \xi_2)^2}{\sigma_z^2}
	\label{eq:mahalonobis}
\end{equation}

\indent{Итак, для решения задачи требуется найти коэффициенты $\alpha_1, \dots, \alpha_p$, минимизарующие $\Delta^2$. Для классификации вектора $X$ будем использовать следующее правило:}

\begin{equation}
	\begin{aligned}
	x_i \in W_1: \sum_{i = 1}^p \alpha_i x_i \ge c + \ln{\frac{q_2}{q_1}} \\
	x_i \in W_2: \sum_{i = 1}^p \alpha_i x_i < c + \ln{\frac{q_2}{q_1}},
	\end{aligned}
	\label{eq:decision_rule}
\end{equation}
где $P(x_i \in W_1) = q_1, \;\; P(x_i \in W_2) = q_2$ –– априорные вероятности

\subsection{Численный эксперимент}
\subsubsection{Модельные данные}

\indent{\indent Проведем два эксперимента на смоделированных данных: в случае, когда данные "хорошо" разделимы и наоборот. Будем считать, что данные "хорошо" разделимы, когда их диапазоны значений векторов средних $\mu$ не пересекаются с учетом дисперсий.} 

\indent{Смоделируем две обучающие выборки объемами $n_1, n_2$ из нормального трехмерного распределения с заданными параметрами: $X_1 \sim N(\mu_1, \sum\nolimits_1), \; X_2 \sim N(\mu_2, \sum\nolimits_2), \; X_1, X_2 \in R^3$. Сгенерируем тестовую выборку, распределенную по трехмерному нормальному распределению, включающую наблюдения из обеих популяций. Объемы выборок $n_1 = 200, \; n_2 = 300$. Приведем значения $\mu_1, \; \mu_2, \; \sum\nolimits$ для "хорошо" разделимых данных}

\begin{table}[htb]   
	\begin{minipage}{.4\linewidth}
		\begin{tabular}{| c | c | c |}
			\hline
			0.518 & 0.167 & -0.582 \\ \hline
			0.167 & 0.958 & -0.091 \\ \hline
		    -0.582 & -0.091 & 0.670 \\
			\hline
		\end{tabular}
		\caption{Матрица ковариаций для "хорошо" разделимых данных}
		\label{tab:well_sigma}
	\end{minipage}
	\hfill
	\begin{minipage}{.4\linewidth}
		\begin{tabular}{| c || c | c | c |}
			\hline
			$\mu_1$ & 4.360 & 4.031 & 4.230 \\ \hline
			$\mu_2$ & 1.036 & 1.031 & 1.343 \\
			\hline
		\end{tabular}
		\caption{Векторы средних для "хорошо" разделимых данных}
		\label{tab:well_mu}
	\end{minipage}
\end{table}

\indent{Сформируем тестовую выборку из смеси данных, распределенных как исходные популяции $W_1, W_2$. Зададим априорные вероятности $q_1 = 0.4, \; q_2 = 0.6$. Объем тестовой выборки $n_{test} = 100$. Представим результат классификации в виде матрицы ошибок:}

\begin{table}[htb]  
    \centering
	\begin{tabular}{|l||*{2}{c|}} \hline
		\backslashbox{Real}{Pred}
		&\makebox[3em]{$W_1$}&\makebox[3em]{$W_2$}\\\hline\hline
		$W_1$ & 40 & 0 \\\hline
		$W_2$ & 0 & 60 \\\hline
	\end{tabular}
	\caption{Матрица ошибок для "хорошо" разделимых данных}
	\label{tab:well_confusion}
\end{table}

\indent{Исходя из заданных $q_1,\; q_2, \; n_{test}$ 	получаем, что в тестовой выборке 40 наблюдений популяции $W_1$ и 60 наблюдений популяции $W_2$. По результатам таблицы \ref{tab:well_confusion} делаем вывод, что все объекты были безошибочно классифицированы.}

\indent{Выполним классификацию данных из 1-ой и 2-ой обучающих выборок с помощью дискриминантной функции, построенной по "хорошо" разделимым данным. Представим матрицы ошибок:}

\clearpage 

\begin{table}[htb]  
	\centering
    %\begin{minipage}{.4\linewidth}
		\begin{tabular}{|l||*{2}{c|}} \hline
			\backslashbox{Real}{Pred}
			&\makebox[3em]{$W_1$}&\makebox[3em]{$W_2$}\\\hline\hline
			$W_1$ & 200 & 0 \\\hline
			$W_2$ & 0 & 300 \\\hline
		\end{tabular}
	\caption{Матрица ошибок для обучающих выборок, ДФ построена по "хорошо" разделимым данным}
	\label{tab:well_confusion_1}
	%\end{minipage}
	%\hfill
	%\begin{minipage}{.4\linewidth}
%		\begin{tabular}{|l||*{2}{c|}} \hline
%			\backslashbox{Real}{Pred}
%			&\makebox[3em]{$W_1$}&\makebox[3em]{$W_2$}\\\hline\hline
%			$W_1$ & 0 & 0 \\\hline
	%		$W_2$ & 0 & 300 \\\hline
		%\end{tabular}
		%\caption{Матрица ошибок для обучающей выборки 2, ДФ построена по "хорошо" %разделимым данным}
	%	\label{tab:well_confusion_2}
	%\end{minipage}
\end{table}

\indent{По результатам из таблицы \ref{tab:well_confusion_1} видим, что модель безошибочно классифицирует обучающие данные.}

\indent{Сгенерируем "плохо" разделимые данные: значения векторов средних $\mu$ пересекаются. Тестовую выборку будем формировать как раньше: $q_1 = 0.4, \; q_2 = 0.6$, $n_{test} = 100, \; n_1 = 200, \; n_2 = 300$. Приведем параметры для генерации данных и матрицу ошибок в виде таблиц:}

\begin{table}[htb]   
	\begin{minipage}{.4\linewidth}
		\begin{tabular}{| c | c | c |}
			\hline
			0.476 & 0.031 & -0.557 \\ \hline
			0.031 & 1.946 & -0.542 \\ \hline
			-0.557 & -0.542 & 0.804 \\
			\hline
		\end{tabular}
		\caption{Матрица ковариаций для "плохо" разделимых данных}
		\label{tab:badly_sigma}
	\end{minipage}
	\hfill
	\begin{minipage}{.4\linewidth}
		\begin{tabular}{| c || c | c | c |}
			\hline
			$\mu_1$ & 1.538 & 1.567 & 1.364 \\ \hline
			$\mu_2$ & 1.365 & 1.171 & 1.245 \\
			\hline
		\end{tabular}
		\caption{Векторы средних для "плохо" разделимых данных}
		\label{tab:badly_mu}
	\end{minipage}
\end{table}

\begin{table}[htb]  
    \centering
	\begin{tabular}{|l||*{2}{c|}} \hline
		\backslashbox{Real}{Pred}
		&\makebox[3em]{$W_1$}&\makebox[3em]{$W_2$}\\\hline\hline
		$W_1$ & 38 & 2 \\\hline
		$W_2$ & 2 & 58 \\\hline
	\end{tabular}
	\caption{Матрица ошибок для "плохо" разделимых данных}
	\label{tab:badly_confusion}
\end{table}

\clearpage 

\indent{По результатам из таблицы \ref{tab:well_confusion} виидим, что из 100 наблюдений тестовой выборки 96 удалось классифицировать верно.}

\indent{Выполним классификацию данных из 1-ой и 2-ой обучающих выборок с помощью дискриминантной функции, построенной по "плохо" разделимым данным. Представим матрицы ошибок:}

\begin{table}[htb]
	\centering  
    %\begin{minipage}{.4\linewidth}
		\begin{tabular}{|l||*{2}{c|}} \hline
			\backslashbox{Real}{Pred}
			&\makebox[3em]{$W_1$}&\makebox[3em]{$W_2$}\\\hline\hline
			$W_1$ & 180 & 20 \\\hline
			$W_2$ & 19 & 281 \\\hline
		\end{tabular}
	\caption{Матрица ошибок для обучающих выборок, ДФ построена по "плохо" разделимым данным}
	\label{tab:badly_confusion_1}
	%\end{minipage}
	%\hfill
	%\begin{minipage}{.4\linewidth}
	%	\begin{tabular}{|l||*{2}{c|}} \hline
	%		\backslashbox{Real}{Pred}
	%		&\makebox[3em]{$W_1$}&\makebox[3em]{$W_2$}\\\hline\hline
	%		$W_1$ & 0 & 0 \\\hline
	%		$W_2$ & 19 & 281 \\\hline
	%	\end{tabular}
	%	\caption{Матрица ошибок для обучающей выборки 2, ДФ построена по "плохо" разделимым данным}
	%	\label{tab:badly_confusion_2}
	%\end{minipage}
\end{table}

\indent{По результатам из таблицы \ref{tab:badly_confusion_1}, видим, что модель явно ошибается при классификации данных из 1-ой выборки. То же самое касается и 2-ой выборки.}

\indent{Вычислим вероятности ошибочной классификации по формулам}

\begin{equation}
	P(2|1) = \Phi\left(\frac{K - 0.5 \cdot \Delta^2}{\Delta}\right)
	\label{eq:prob_2_1}
\end{equation}

\begin{equation}
	P(1|2) = \Phi\left(\frac{-K - 0.5 \cdot \Delta^2}{\Delta}\right),
	\label{eq:prob_1_2}
\end{equation}
где $K = \ln{\frac{q_2}{q_1}}$

\clearpage

\begin{table}[htb]   
	\centering
	\begin{tabular}{| c || c | c |}
		\hline
		$ $ & Well & Badly \\ \hline\hline
		$P(2|1)$ & $0$ & 0.027 \\ \hline
		$P(1|2)$ & $0$ & 0.016 \\
		\hline
	\end{tabular}
	\caption{Вероятности ошибок для моделей, построенных на "хорошо" и "плохо" разделимых данных}
	\label{tab:probs_well_badly}
\end{table}

\indent{Исходя из результатов в таблице \ref{tab:probs_well_badly}, можно сделать вывод, что результаты классификации обучающих выборок согласуются с вероятностями допущения ошибок. Общая доля неверных ответов на тестовых данных –– 0.0 для "хорошей" модели, 0.04 для "плохой" модели.}

\subsubsection{Данные из репозитория}

\indent{\indent Исходные данные представлены 24 признаками, количество наблюдений $n = 1000$. Также данные разбиваются на две популяции $D_1,\; D_2$, что представлено последним признаком, принимающим значения $\{1, 2\}$. Исходя из предположений о нормальности расперделений популяций, воспользуемся оценками параметров распределений для построения дискриминантной функции. Формулы для вычисления оценок приведены ниже: }

\begin{equation}
	\hat{\mu_j}^{(k)} = \frac{1}{n_k} \sum_{i=1}^{n_k} x_{ij}^{(k)}, \;\; k = 1, 2
	\label{eq:mu_est}
\end{equation}

\begin{equation}
	\begin{gathered}
		S = \frac{1}{n_1 + n_2 - 2}\left[(n_1 - 1)S^{(1)} + (n_2 - 1)S^{(2)}\right] \\
		S^{(k)} = \left(s_{lj}^{(k)}\right), \;\; l, j = 1, \dots, p, \;\; k = 1, 2 \\
		s_{lj}^{(k)} = \frac{1}{n_k - 1} \sum_{i=1}^{n_k} (x_{il}^{(k)} - \hat{\mu_l}^{(k)}) (x_{ij}^{(k)} - \hat{\mu_j}^{(k)}), \;\; k = 1, 2 
	\end{gathered}
	\label{eq:sigma_est}
\end{equation}

\indent{Вектор параметров дискриминантной функции $\alpha$ заменим соответствующей оценкой $a$:}

\begin{equation}
	a = S^{-1}(\hat{\mu}^{(1)} - \hat{\mu}^{(2)})
	\label{eq:alpha_est}
\end{equation}

\indent{Также заменим $\xi_1,\; \xi_2, \; \sigma_z^2$ соответствующими оценками:}

\begin{equation}
	\xi_i \rightarrow \overline{z_i} = \frac{1}{n_i} \sum_{j=1}^{n_i} z_j^{(i)}, \;\; i = 1, 2
	\label{eq:xi_est}
\end{equation}

\begin{equation}
	\sigma_z^2 \rightarrow s_z^2 = \sum_{l=1}^p \sum_{j=1}^p a_l s_{lj} a_j 
	\label{eq:sig_est}
\end{equation}

\indent{Запишем новый вид решающего правила, формулы для вычисления смещенной и несмещенной оценок расстояния Махаланобиса, вероятностей ошибочной классификации:}

\begin{equation}
	\begin{aligned}
	x_i \in D_1: \sum_{i = 1}^p a_i x_i \ge \frac{\overline{z_1} + \overline{z_2}}{2} + \ln{\frac{q_2}{q_1}} \\
	x_i \in D_2: \sum_{i = 1}^p a_i x_i < \frac{\overline{z_1} + \overline{z_2}}{2} + \ln{\frac{q_2}{q_1}},
	\end{aligned}
	\label{eq:decision_rule_est}
\end{equation}

\begin{equation}
	D^2 = \frac{(\overline{z_1} - \overline{z_2})^2}{s_z^2}
	\label{eq:mah_est}
\end{equation}

\begin{equation}
	D_H^2 = \frac{n_1 + n_2 - p - 3}{n_1 + n_2 - 2}D^2 - p\left(\frac{1}{n_1} + \frac{1}{n_2}\right)
	\label{eq:mah_unbiased_est}
\end{equation}

\vspace{1cm}

\indent{Число наблюдений каждой популяций в выборке равны $n_1 = 700, \; n_2 = 300$ соответственно. Заметим, что значения признаков лежат в разных интервалах. Для получения качественного результата классификации необходимо провести стандартизацию данных.}

\indent{Разделим данные на обучающие и тестовые в соотношении 5:1 так, чтобы в обоих наборах сохранялось соотношение между числом объектов 1-ой и 2-ой популяций. Используя тестовые данные, найдем оценки параметров распределения, построим дискриминантную функцию, найдем остальные необходимые оценки и проведем классификацию на тестовых и тренировочных данных. Представим результаты в виде матриц ошибок:}

\clearpage

\begin{table}[htb]  
    \begin{minipage}{.4\linewidth}
		\begin{tabular}{|l||*{2}{c|}} \hline
			\backslashbox{Real}{Pred}
			&\makebox[3em]{$D_1$}&\makebox[3em]{$D_2$}\\\hline\hline
			$D_1$ & 120 & 20 \\\hline
			$D_2$ & 27 & 33 \\\hline
		\end{tabular}
	\caption{Матрица ошибок для данных из репозитория, тестовая выборка}
	\label{tab:credit_confusion_1}
	\end{minipage}
	\hfill
	\begin{minipage}{.4\linewidth}
		\begin{tabular}{|l||*{2}{c|}} \hline
			\backslashbox{Real}{Pred}
			&\makebox[3em]{$D_1$}&\makebox[3em]{$D_2$}\\\hline\hline
			$D_1$ & 507 & 53 \\\hline
			$D_2$ & 111 & 129 \\\hline
		\end{tabular}
		\caption{Матрица ошибок для данных из репозитория, тренировочная выборка}
		\label{tab:credit_confusion_2}
	\end{minipage}
\end{table}

\indent{Вычислим вероятности ошибок по формулам:}

\begin{equation}
	\hat{P}(2|1) = \Phi\left(\frac{K - 0.5 \cdot D_H^2}{D_H}\right)
	\label{eq:prob_2_1_est}
\end{equation}

\begin{equation}
	\hat{P}(1|2) = \Phi\left(\frac{-K - 0.5 \cdot D_H^2}{D_H}\right),
	\label{eq:prob_1_2_est}
\end{equation}
где $K = \ln{\frac{q_2}{q_1}}$

\indent{Получили, что $\hat{P}(2|1) \sim 0.096, \;\; \hat{P}(1|2) \sim 0.534$. Вероятность отнести объект к классу 2 при условии, что он принадлежит к 1, довольно высока. Общая доля неверных ответов на тестовых данных –– 0.235.}

\clearpage

\section*{Заключение}
\addcontentsline{toc}{section}{Заключение}

\indent{\indent В ходе работы были проведены эксперименты как на модельных, так и на реальных данных.}

\indent{По результатам на модельных данных можно заключить, что данные хорошо классифицируются, если выборки явно разделимы, то есть, параметры их распределений сильно отличаются. Заметим, что, сблизив интервалы значений векторов средних, мы добились ухудшения классификации, хотя и не критичного.}

\indent{Модель для реальных данных строится на основе выборочных оценок распределения, поскольку параметры распределения неизвестны. Из этого сделует, что идеальная классификация на реальных данных невозможна. В результате эксперимента получили, что оценка вероятности ошибки $\hat{P}(1|2)$ довольно высока. Это может быть связано с тем, что исходные данные не сбалансированы по классам, и объектов класса 1 в обучающей выборке гораздо больше.}

\indent{К минусам использования дискриминантной функции для классификации можно отнести то, что решающее правило включает значения априорных вероятностей. Возможно, оценки вероятностей, вычисленные по обучающим выборкам, не всегда отражают разделение в других выборках. Поэтому, вероятно, этот метод не подходит для малых выборок, по крайней мере, в той форме, в которой он рассмотрен в данной работе.}

\clearpage

\bibliographystyle{splncs04}
\bibliography{mybibliography}


\end{document}{}